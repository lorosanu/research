\documentclass[xcolor=table]{beamer}

\uselanguage{English}
\languagepath{English}
\usetheme{HongKong}
\useinnertheme{rounded}
\usepackage{color, colortbl}
\usepackage{tcolorbox}
\usepackage{multirow} 
\usepackage{textcomp}
\usepackage{siunitx}
\usepackage{tikz}
\usepackage[utf8]{inputenc}
\usepackage[safe]{tipa}
\usepackage{array}
\usepackage{hhline}
\usepackage{pbox}

\usepackage{amsmath}


\usepackage{etoolbox}

\usepackage{algorithm,algorithmic}
%\usepackage[noend]{algpseudocode}

\tikzset{rndblock/.style={rounded corners,rectangle,draw,outer sep=0pt}}
\newcommand{\tframed}[2][]{\tikz[baseline=0.1pt]\vspace{0.1cm}\node[rndblock,minimum height=1.5em,#1] (m) {#2} ;}
\newcommand{\hilight}[1]{\textbf{\tframed[blue,fill=blue!10]{#1}}}
\newcommand{\whilight}[1]{\textbf{\tframed[purple,fill=purple!10]{#1}}}

\newcommand{\sP}{\hspace{1pt}}
\newcommand{\mP}{\hspace{3pt}}
\newcommand{\bP}{\hspace{6pt}}
\newcommand{\BP}{\hspace{12pt}}


\newcolumntype{L}[1]{>{\raggedleft\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}

%\newcommand{\hilight}[1]{\colorbox{white}{#1}}

%==============================================================================================================

\setbeamertemplate{navigation symbols}{}
\setbeamertemplate{items}[ball] 
\setbeamertemplate{blocks}[rounded][shadow=false,width=.7\textwidth] 

\setbeamercolor{myBlueBox} {bg=blendedblue, fg=white}
\setbeamercolor{myGrayBox} {bg=blendedgray, fg=black}

\definecolor{darksalmon}{RGB}{233,150,122}
\definecolor{blendedblue}{rgb}{0.137,0.466,0.741}
\definecolor{blendedgray}{rgb}{0.838,0.833,0.833}
\definecolor{blendedpurple}{RGB}{75,20,130}
\definecolor{tgray}{rgb}{0.211, 0.211,0.244}
\definecolor{darkgray}{rgb}{0.450,0.450,0.450}
\definecolor{maroon}{rgb}{0.665, 0.142, 0.142}
\definecolor{ngreen}{rgb}{0.000,0.500,0.000}

\newcolumntype{g}{>{\columncolor{tgray}}S[tabformat=2.2]}

\newenvironment<>{varblock}[2][\textwidth]{%
\vspace*{-20pt}
	  \setlength{\textwidth}{#1}
	  \begin{actionenv}#3%
	    \def\insertblocktitle{#2}%
	    \par%
	    \usebeamertemplate{block begin}}
	  {\par%
	    \usebeamertemplate{block end}%
	  \end{actionenv}
\vspace*{-20pt}
}


%\setbeamertemplate{itemize item}{$\blacktriangleright$}
\setbeamertemplate{itemize subitem}{$\ast$}
\setbeamerfont{itemize/enumerate subbody}{size=\footnotesize} 							%to set the body size
%\setbeamertemplate{itemize subitem}{\tiny\raise1.25pt\hbox{\donotcoloroutermaths$\blacktriangleright$}}  	%to set the symbol size


%==============================================================================================================

\title{Adding new words into a language model using parameters of known words with similar behavior}
\author[Luiza Orosanu, Denis Jouvet]
{
\vskip0.3cm \textbf{Luiza Orosanu, Denis Jouvet}
\vskip0.45cm  INRIA-Loria, Nancy, France  \\
\vskip0.15cm Multispeech Team\vspace{-1cm}
}

\date{}

\AtBeginSection[]
{
  \begin{frame}
	\thispagestyle{empty}
   	\frametitle{Summary}
	\tableofcontents[currentsection, hideothersubsections,sectionstyle=show/shaded, subsectionstyle=show/shaded/hide] 	%subsectionstyle=show/shaded,currentsection,subsections, currentsubsection]
  \end{frame}
}


\AtBeginSubsection[]
{
  \begin{frame}
	\thispagestyle{empty}
   	\frametitle{Summary}
	\tableofcontents[currentsubsection, hideothersubsections,sectionstyle=show/shaded, subsectionstyle=show/shaded/hide] 	
  \end{frame}
}


\newcommand{\firstslide}{%
	\begin{frame}
	\hfill
	\thispagestyle{empty}
	\titlepage
	\end{frame}
}

%==============================================================================================================
\begin{document}
\renewcommand{\inserttotalframenumber}{15}

%==============================================================================================================
\firstslide


%==============================================================================================================
\section{Context}
\begin{frame}{Context}
\setcounter{framenumber}{1}

\begin{itemize}
\item Context
	\begin{itemize}
	\smallskip
	\item language models in automatic speech recognition systems
	\smallskip
	\item trained on large text data sets
	\smallskip
	\item having closed vocabulary generating OOV problems
	\end{itemize}

\vskip0.7cm
\item Our study
	\begin{itemize}
	\item add new words that are specific to a certain domain 
	\vskip0.1cm	
	\item avoid recognition errors of words that are frequently pronounced (yet unknown by the system)
	\end{itemize}
\end{itemize}

\end{frame}

%==============================================================================================================
\begin{frame}{Context}

\begin{itemize}
\item \textbf{\color{purple} Our approach}

	\begin{itemize}	
	\vskip0.5cm	
	\item without retraining or adapting the model \\(which requires a lot of new data relative to the new words)
	\vskip0.5cm	
	\item approach based on the similarity with in-vocabulary words \\
		\vskip1ex\hskip0.3cmtwo words are \textbf{\color{purple}similar} if they appear in similar contexts \\
		\hskip0.5cm{\footnotesize $\iff$ they have similar neighbor distributions}
	\end{itemize}
	
\end{itemize}

\end{frame}



%==============================================================================================================
\section{Approach}
\begin{frame}{Approach}
\setcounter{framenumber}{3}

\begin{itemize}
\vskip2ex
\item use a few examples of sentences for each new word
\vskip2ex
\item find similar known words (similar neighbor distributions) 
\vskip2ex
\item transpose LM probabilities from similar words to new words
\end{itemize}


\end{frame}

%==============================================================================================================
\begin{frame}[t]{Approach}

\vskip0.3cm
\only<1->
{
	\begin{itemize}
	\item[\textbf{\color{blendedblue}1.}] Acquire a few \textbf{\color{blendedblue}examples of sentences} with the new word

		\vskip1ex	
		\hskip-0.5ex $\rightarrow$ compute the neighbor distributions of the new word \textit{nW}
		\begin{center}$P_k(w|\textbf{\color{purple}nW}) \text{ for } k \in \{..., -2, -1, +1, +2, ...\}$\end{center}
	\end{itemize}
}

\vskip0.5cm

\only<2->
{
\footnotesize
\vskip-1ex\rule{\textwidth}{1.15pt}

\vskip-1ex
\begin{itemize}
\item example of new word : \textbf{\color{purple}soir}
\end{itemize}


\vskip-1ex
\begin{itemize}
\item examples of sentences	
\end{itemize}

\begin{center}
{\scriptsize
\begin{tabular}{rcl}
on ignorait \underline{encore}  \underline{lundi} 	& \hskip-2.5ex\textbf{\color{purple}soir} & \hskip-2.5ex\underline{les} \underline{conditions} de sa survie \\
devine qui vient \underline{dîner}  \underline{ce}    	& \hskip-2.5ex\textbf{\color{purple}soir} &   \\
pas de consigne de \underline{vote} \underline{au} 	& \hskip-2.5ex\textbf{\color{purple}soir} & \hskip-2.5ex\underline{du} \underline{premier} tour \\
\end{tabular}	
}
\end{center}

\vskip-2ex
\begin{itemize}
\item preceeding and following neighbors
\end{itemize}

	{\tiny
	\begin{tabular}{l|l}
	k= -2  	& encore, dîner, vote,  ... 	\\
	k= -1  	& lundi, ce, au, ... 		\\
	k= +1  	& les, du, ...    		\\
	k= +2  	& conditions, premier, ... 	\\
	\end{tabular}
	}
}

\end{frame}

%==============================================================================================================
\begin{frame}[t]{Approach}

\vskip0.3cm
\only<1->
{
	\begin{itemize}
	\item[\textbf{\color{blendedblue}2.}] Search for similar words in a \textbf{\color{blendedblue}reference corpus} 

		\vskip1ex	
		\hskip-0.5ex $\rightarrow$ compute the neighbor distributions of each known word \textit{kW}
		\begin{center}$P_k(w'|\textbf{\color{ngreen}kW}) \text{ for } k \in \{..., -2, -1, +1, +2, ...\}$\end{center}
	\end{itemize}
}

\vskip0.05cm

\only<2->
{
\footnotesize
\vskip-1ex\rule{\textwidth}{1.15pt}

\begin{itemize}
\item use directly the counts file of n-gram sequences	
	\begin{itemize}	
	\item {\tiny 2g $\Rightarrow$ maximum 2 neighbors $k \in \{-1,+1\}$ }
	\item {\tiny 3g $\Rightarrow$ maximum 4 neighbors $k \in \{-2,-1,+1,+2\}$ }
	\end{itemize}	

\vskip1ex
\item examples of 3-gram entries with the known word '\textbf{\color{ngreen}matin}'
	\vskip-1ex
	{\scriptsize 
	\begin{itemize}
	\item {\scriptsize\fontfamily{qcr}\selectfont "beau \textbf{\color{ngreen}matin} de \hskip4.5ex\textit{9}"} 	\hskip1ex {\tiny $\rightarrow$ k=-1 neighbor "beau"; k=+1 neighbor "de"}
	\item {\scriptsize\fontfamily{qcr}\selectfont "\textbf{\color{ngreen}matin} a été \hskip6ex\textit{10}"}  	\hskip1ex {\tiny $\rightarrow$ k=+1 neighbor "a"; k=+2 neighbor "été"}
	\item {\scriptsize\fontfamily{qcr}\selectfont "jusqu' au \textbf{\color{ngreen}matin} \textit{28}"} \hskip1.1ex {\tiny $\rightarrow$ k=-2 neighbor "jusqu'"; k=-1 neighbor "au"}
	\end{itemize}
	}
\vskip1ex
\item preceeding and following neighbors
	{\tiny
	\begin{tabular}{l|l}
	k= -2  	& jusqu', ... 		\\
	k= -1  	& beau, au, ... 	\\
	k= +1  	& de, a, ...    	\\
	k= +2  	& été, ... 		\\
	\end{tabular}
	}
\end{itemize}

}	

\end{frame}


%==============================================================================================================
\begin{frame}[t]{Approach}

\vskip0.8cm
\only<1->
{
	\begin{itemize}
	\item[\textbf{\color{blendedblue}3.}] Compute the \textbf{\color{blendedblue}KL divergence} of neighbor distributions \\
		\hskip5ex $\rightarrow$  between each known word (\textit{kW}) and a new word (\textit{nW})
	\end{itemize}

	\vskip1ex	
	{\footnotesize
	\begin{center}$D_{KL} \left( \ P_{k}(\bullet|\textbf{\color{ngreen}kW}) \ || \ {P_{k}(\bullet|\textbf{\color{purple}nW})}\ \right) = \sum\limits_{w}^{ } \  P_{k}(w|\textbf{\color{ngreen}kW}) \ log \left( \frac{P_{k}(w|\textbf{\color{ngreen}kW})}{P_{k}(w|\textbf{\color{purple}nW})} \right)$\end{center}
	
	\begin{center}
	$D(\textbf{\color{ngreen}kW},\textbf{\color{purple}nW}) = \sum\limits_{k} D_{k}(\textbf{\color{ngreen}kW},\textbf{\color{purple}nW})$
	\end{center}
	}
}

\smallskip
\only<2->
{
	\begin{itemize}
	\item[\textbf{\color{blendedblue}4.}] Select \textbf{\color{blendedblue}the most similar words} to a new word 
	
		\vskip1ex	
		\hskip3ex $\rightarrow$ those having minimal divergences
	\end{itemize}
}

\end{frame}


%==============================================================================================================
\begin{frame}[t]{Approach}

\vskip0.9cm
\begin{itemize}
\item[\color{blendedblue}$\Rightarrow$] examples of similar words
\end{itemize}

\vskip2ex
{\scriptsize
\begin{tabular}{p{1.3cm}p{0.5cm}p{7cm}}
& soir		& $\rightarrow$ matin, midi, dimanche, samedi, vendredi	\\	
& soirs		& $\rightarrow$ temps, joueurs, matchs, pays, matches	 	\\
\end{tabular}
}

	
\vskip3ex
{\scriptsize
\begin{tabular}{p{1.05cm}p{0.75cm}p{7cm}}
& année		& $\rightarrow$ époque, opération, expérience, épreuve, édition	\\	
& années 	& $\rightarrow$ décennies, saisons, épisodes, heures, opérations	\\
\end{tabular}
}


\vskip3ex
{\scriptsize
\begin{tabular}{p{0.005cm}p{1.8cm}p{7cm}}
& gouvernement 	& $\rightarrow$ parti, président, peuple, roi, mouvement 	\\
& gouvernements	& $\rightarrow$ ministres, partis, syndicats, services, pays	\\
\end{tabular}
}


\vskip3ex
{\scriptsize
\begin{tabular}{p{1.0cm}p{0.8cm}p{7cm}}
& guerre 	& $\rightarrow$ campagne, crise, paix, position, ville		\\	
& guerres 	& $\rightarrow$ combats, opérations, missions, campagnes, séries	\\
\end{tabular}
}

\end{frame} 



%==============================================================================================================
\begin{frame}[t]{Approach}

\vskip0.4cm
\begin{itemize}
\item[\textbf{\color{blendedblue}5.}] \textbf{\color{blendedblue}Transpose the probabilities} of similar words onto new words

	\begin{itemize}
	\vskip1ex
	\item[$\rightarrow$] look for n-grams containing the similar words
	
	\vskip1ex
	\item[$\rightarrow$] replace the 'similar words' by the 'new word'
	
	\vskip1ex
	\item[$\rightarrow$] add the new n-grams into the new language model 
	\end{itemize}
\end{itemize}

\vskip0.2cm

\only<2->
{
\footnotesize
\vskip-1ex\rule{\textwidth}{1.15pt}


\begin{itemize}
\smallskip
\item new word "\textbf{\color{purple}soir}" similar to the known word "\textbf{\color{ngreen}matin}"

\vskip2.5ex
\item \underline{known n-grams} (in the language model) \\
	{\scriptsize
	\vskip1ex\hskip5ex {\scriptsize\fontfamily{qcr}\selectfont"-1.48214 possible ce \textbf{\color{ngreen}matin}"} \\
	\vskip1ex\hskip5ex {\scriptsize\fontfamily{qcr}\selectfont"-1.404164 \textbf{\color{ngreen}matin} ajoute que"}
	}
	
\vskip2.5ex
\item \underline{new n-grams} (to add in the new LM) \\
	{\scriptsize
	\vskip1ex\hskip5ex {\scriptsize\fontfamily{qcr}\selectfont"-1.48214 possible ce \textbf{\color{purple}soir}"} \\
	\vskip1ex\hskip5ex {\scriptsize\fontfamily{qcr}\selectfont"-1.404164 \textbf{\color{purple}soir} ajoute que"}
	}

\end{itemize}

}
	
\end{frame}



%==============================================================================================================
\section{Experiments}
\subsection{Setup for experiments}
\begin{frame}[t]{Setup for experiments}
\setcounter{framenumber}{9}

\vskip0.5cm

\begin{itemize}
\item Select a list of new words to add to a language model \\ \hskip0.5cm $\Rightarrow$ \textbf{\color{purple}44 new words} 

\vskip0.5cm
\item Search for similar words
	\begin{itemize}
	
	\vskip0.1cm
	\item configuration \\
		\smallskip
		\hskip0.5cm - sentences based on "word$|$part-of-speech" units \\
		
		\smallskip
		\hskip0.5cm - 4 neighbor positions for each word: $k \in \{-2, -1, +1, +2\}$ \\
		
		\smallskip
		\hskip0.5cm - choose the 10 most similar words for each new word

	\vskip0.3cm
	\item evaluate the impact of using \{5, 10, 20 or 50\} examples of sentences for each new word
	\end{itemize}	
\end{itemize}

\vskip2ex
\begin{columns}
\begin{column}{0.1\textwidth}
\end{column}
\begin{column}{0.99\textwidth}
\begin{beamerboxesrounded}[width=0.90\textwidth,shadow=false]{\footnotesize Sentences based on "word$|$part-of-speech" units }	
{\scriptsize 
\renewcommand{\arraystretch}{1.2}
	\begin{tabular}{rrrrrr}
	qui 			 & vient 		& dîner 		& ce 		& soir	\\
	\hskip-3ex PRO:REL$|$qui & VER:pres$|$vient 	& VER:infi$|$dîner 	& PRO:DEM$|$ce 	& NOM$|$soir \\
	\end{tabular}
}
\end{beamerboxesrounded}
\end{column}
\end{columns}

\end{frame}



%==============================================================================================================
\begin{frame}[t]{Setup for experiments}

\begin{itemize}

\vskip0.4cm
\item The \textbf{\color{blendedblue}BASELINE} language model 

	\smallskip
	\begin{itemize}
	\item[\color{blue}$\ast$] large vocabulary language model 
	\smallskip
	\item[\color{blue}$\ast$] trained by interpolation on various textual data %\\\hskip2ex (1.3B words, 58M sentences)
	\smallskip
	\item[\color{blue}$\ast$] does not know the 44 new words
	\end{itemize}	
	
\vskip0.4cm	
\item The \textbf{\color{blendedblue}ORACLE} language model 

	\smallskip
	\begin{itemize}
	\item[\color{blue}$\ast$] large vocabulary language model 
	\smallskip
	\item[\color{blue}$\ast$] trained by interpolation on various textual data %\\\hskip2ex (1.7B words, 74M sentences)
	\smallskip
	\item[\color{blue}$\ast$] knows the 44 new words
	\end{itemize}	
\end{itemize}

\vskip0.2cm

\begin{table}[h]
\begin{tabular}{|r|r|r|r|}
\hline
LM  		& 1-grams	& 2-grams	& 3-grams 	\\ \hline
BASELINE  	& 97 305	& 42.9M 	& 79.2M 	\\
ORACLE  	& 97 349	& 43.3M		& 80.1M 	\\ \hline
\end{tabular}
\end{table}


\end{frame}

%==============================================================================================================
\begin{frame}[t]{Setup for experiments}

\begin{itemize}

\vskip0.5cm
\item The \textbf{\color{blendedblue}new} language models created
	\begin{itemize}
	
	\smallskip
	\item[\color{blue}$\ast$] by using \{5, 10, 20, 50\} examples of sentences for each new word
	
	\smallskip
	\item[\color{blue}$\ast$] by adding 1-grams or 1-,2-,3-grams of new words \\\hskip3ex into the BASELINE LM 			
	\end{itemize}	
\end{itemize}

\vskip0.4cm
\begin{center}
\begin{table}[h]
\scriptsize
\begin{tabular}{|r|r|r|r|r|r|r|}
\cline{2-7}
\multicolumn{1}{c|}{} 	& \multirow{2}{*}{\textbf{baseline}}	& \multicolumn{4}{c|}{\textbf{new language models}} & \multirow{2}{*}{\textbf{ORACLE}} \\ \cline{3-6}
\multicolumn{1}{c|}{} 	& 				& 5ex	& 10ex	& 20ex		& 50ex			& 		\\ \hline
\# 2-grams  		& 42.9				& 44.7	& 44.6  & 44.8 		& 44.8			& 43.3 	\\
\# 3-grams		& 79.2				& 89.8	& 89.3	& 90.5 		& 90.8			& 80.1		\\ \hline
\end{tabular}
\caption{\scriptsize \centering Number [in millions] of 2-grams and 3-grams in the new 'baseline+1-,2-,3-grams' LMs}
\end{table}
\end{center}

\vskip1ex
{\scriptsize
{\color{blendedblue}$\Rightarrow$} The new 'baseline+1-,2-,3-grams' adds:
	\begin{itemize}
	\smallskip
	\item[\color{blue}$\ast$] between 1.7M and 1.9M new 2-grams 
	
	\smallskip
	\item[\color{blue}$\ast$] between 10.6M and 11.6M new 3-grams 			
	\end{itemize}	
}


\end{frame}


%==============================================================================================================
\begin{frame}[t]{Setup for experiments}

\vskip0.8cm	
\begin{itemize}
\item Setup for evaluations \\
	\smallskip
	\hskip0.3cm - the LMs are evaluated over the ESTER2 development set \\
	\smallskip
	\hskip0.3cm - the 44 new words have an occurrence frequency of 1.33\% \\
	
\vskip0.8cm	
\item Compare the performance of new LMs with baseline LM \\
	\smallskip
	\hskip0.3cm - regarding the WER  \\
	\smallskip
	\hskip0.3cm - regarding the percentage of new words correctly recognized \\
		
\end{itemize}	


\end{frame}		
		
%==============================================================================================================
\subsection{Results}
\begin{frame}[t]{Results: the \textbf{\color{white}WER} performances}
\setcounter{framenumber}{13}

\begin{columns}
\begin{column}{0.4\textwidth}
\begin{table}[h]
\begin{tabular}{rl}
BASELINE  	& \textbf{\color{blendedblue}26.79\%}  \\
ORACLE  	& \textbf{\color{blendedblue}24.79\%} \\
\end{tabular}
\end{table}
\end{column}
\begin{column}{0.5\textwidth}
\end{column}
\end{columns}


\vskip0.1cm

\begin{center}
\begin{table}[h]
\begin{tabular}{|r|r|r|r|r|}
\cline{2-5}
\multicolumn{1}{c|}{} 	& \multicolumn{4}{c|}{\# examples of sentences} 		\\ \cline{2-5}
\multicolumn{1}{c|}{} 	& 5ex		& 10ex		& 20ex		& 50ex		\\ \hline
baseline+1-grams	& 26.45		& 26.44 	& 26.40 	& 26.42		\\
baseline+1-,2-,3-grams	& 25.68		& {\color{blendedblue}25.51}	& {\color{blendedblue}25.51} 	& 25.57		\\ \hline
\end{tabular}
\caption{\footnotesize \centering WER of new 'baseline+1-grams' and 'baseline+1-,2-,3-grams' LMs}
\end{table}
\end{center}


\vskip0.1cm

\only<2->
{
	\scriptsize
	\hskip-5ex{\color{blendedblue}$\Rightarrow$} adding only 1-grams for new words hardly improves the performance \\
	
	\smallskip
	\hskip-5ex{\color{blendedblue}$\Rightarrow$} adding 1-,2-,3-grams for new words provides results closer to the ORACLE's performance \\

	\smallskip
	\hskip-5ex{\color{blendedblue}$\Rightarrow$}  between 1.1\% and 1.3\% WER absolute reduction (compared to the baseline LM)
	
	\smallskip
	\hskip-5ex{\color{blendedblue}$\Rightarrow$}  0.7\% WER difference with the ORACLE model
}


\end{frame}

%==============================================================================================================
\begin{frame}[t]{Results: percentage of {\color{white}new words correctly recognized} }

\begin{columns}
\begin{column}{0.4\textwidth}
\begin{table}[h]
\begin{tabular}{rl}
BASELINE  	& \textbf{\color{blendedblue}0.00\%}  \\
ORACLE  	& \textbf{\color{blendedblue}85.45\%} \\
\end{tabular}
\end{table}
\end{column}
\begin{column}{0.5\textwidth}
\end{column}
\end{columns}

\vskip0.1cm

\begin{center}
\begin{table}[h]
\begin{tabular}{|r|r|r|r|r|}
\cline{2-5}
\multicolumn{1}{c|}{} 	& \multicolumn{4}{c|}{\# examples of sentences} 			\\ \cline{2-5}
\multicolumn{1}{c|}{} 	& 5ex		& 10ex		& 20ex		& 50ex			\\ \hline
baseline+1-grams	& 29.81		& 20.00 	& 22.18 	& 20.36			\\
baseline+1-,2-,3-grams	& 60.54		& 61.81		& {\color{blendedblue}64.90} 	& 62.76	\\ \hline
\end{tabular}
\caption{\footnotesize \centering Correct recognition of new words of new 'baseline+1-grams' and 'baseline+1-,2-,3-grams' LMs}
\end{table}
\end{center}


\vskip0.1cm

\only<2->
{
	\smallskip
	{\scriptsize  {\color{blendedblue}$\Rightarrow$} up to 65\% of the new words correctly recognized }
}


\end{frame}



%==============================================================================================================
\section{Conclusions and future work}

%==============================================================================================================
\begin{frame}[t]{Conclusions and future work}
\setcounter{framenumber}{15}

\vskip0.8cm

\begin{itemize}
\item Conclusions 
	\begin{itemize}
	\smallskip
	\item adding only 1-grams for new words hardly improves the performance

	\smallskip
	\item adding 1-,2-,3-grams for new words provides results closer to the ORACLE's performance

	\smallskip
	\item the similarity approach and the proposed method to add new n-grams into a language model are efficient
	\end{itemize}

\vskip0.8cm
\item Investigate further 
	\begin{itemize}
	\smallskip
	\item the setups for finding similar words 
	\smallskip 
	\item filter the n-grams of new words (diminish the size of new LMs)
	\smallskip 
	\item consider a different number of similar words for each new word
	\end{itemize}
\end{itemize}


\end{frame}


%==============================================================================================================
\begin{frame}[plain]

\begin{center}
\textcolor{polyured}{\huge \textbf{Thank you \\\vskip0.2cm for your attention !}}
\end{center}

\end{frame} 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Annexes %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\setbeamertemplate{footline}[page number]{}
\setbeamertemplate{footline}{}
%==============================================================================================================
\begin{frame}{Annexe}

\scriptsize 
	\begin{itemize}
	\item[\textbf{1.}] aquire a few \textbf{\color{purple}examples of sentences} with the new word

		\vskip1ex	
		\hskip3ex $\rightarrow$ compute the neighbor distributions of the new word $nW$
		\begin{center}$P_k(w|\textbf{\color{blendedblue}nW}) \text{ for } k=\{..., -3, -2, -1, +1, +2, +3, ...\}$\end{center}
	\end{itemize}


\scriptsize
\vskip-1ex\rule{\textwidth}{1.15pt}

\begin{itemize}

\vskip-1ex
\item example of new word : \textbf{\color{purple}tournevis}

\vskip-1ex
\item examples of sentences	
	\begin{itemize}	
	\item {\tiny \underline{le} \textbf{\color{purple}tournevis} \underline{motorisé} \underline{s'} appelle une visseuse}
	\item {\tiny \underline{un} \textbf{\color{purple}tournevis} \underline{suffit} \underline{pour} le démontage}
	\item {\tiny l' \underline{embout} \underline{du} \textbf{\color{purple}tournevis} \underline{peut} \underline{vriller} si on serre trop fort}
	\item {\tiny la \underline{tête} \underline{du} \textbf{\color{purple}tournevis} \underline{peut} \underline{être} plate cruciforme ou autre}
	\item {\tiny ...}	
	\end{itemize}	

\vskip-1ex
\item preceeding and following neighbors
	{\tiny
	\begin{tabular}{l|l}
	p-2 [\#13] 	& (tête \#1) (embout \#1) (a \#1) (manche \#1) (poignées \#1)   ... 						\\
	p-1 [\#4] 	& (le \#6) (un \#6) (du \#3) (de \#3) 										\\
	p+1 [\#13] 	& (motorisé \#1) (suffit \#1) (peut \#2) (et \#2) (standard \#1) (cruciforme \#1)(plat \#1) ...    		\\
	p+2 [\#13] 	& (s' \#1) (pour \#1) (peut \#1) (être \#1) (aux \#1) (est \#1) (exercer \#1) (un \#2) (vriller \#1) ... 	\\
	\end{tabular}
	}

\vskip1ex
\item {\scriptsize the [p-1] neighbor distribution of new word "tournevis"}
	{\tiny
	\begin{tabular}{l|l|l|l|l|l}
				& [p-1]	& le 	&	un 	&	du 	&	de	\\ \hline
	\textbf{nW=tournevis}	& \#18	& 0.333 &	0.333 	&	0.167 	&	0.167	\\
	\end{tabular}
	}
\end{itemize}

\end{frame}


%==============================================================================================================
\begin{frame}[t]{Annexe}


\scriptsize 
	\begin{itemize}
	\item[\textbf{2.}] use \textbf{\color{purple}reference corpus} to search for similar words

		\vskip1ex	
		\hskip3ex $\rightarrow$ compute the neighbor distributions of each known word $kW$
		\begin{center}$P_k(w'|\textbf{\color{blendedblue}kW}) \text{ for } k=\{..., -3, -2, -1, +1, +2, +3, ...\}$\end{center}
	\end{itemize}


\vskip-1ex\rule{\textwidth}{1.15pt}

\scriptsize 
\begin{itemize}
\item use directly the counts file of n-gram sequences	
	\begin{itemize}	
	\item {\tiny 2g $\Rightarrow$ maximum 2 neighbors [p-1], [p+1]}
	\item {\tiny 3g $\Rightarrow$ maximum 4 neighbors [p-2], [p-1], [p+1], [p+2]}
	\end{itemize}	

\vskip-1ex
\item an exemple of a 3-gram entry: "du monde numérique 3"
	\vskip-1ex
	\begin{itemize}
	\item {\scriptsize the known word "monde"}
   		\begin{list}{$\rightarrow$}{\leftmargin=5mm \itemindent=0em}
		\item {\tiny previous neighbor [p-1] : du }
		\item {\tiny following neighbor [p+1] : numérique}
		\end{list}	
	\end{itemize}

\vskip-1ex
\item {\scriptsize preceeding and following neighbors of word "monde"}
	{\tiny
	\begin{tabular}{l|l}
	p-2 [\#685]	& (page \#1) (ordre \#1) (mettre 1) (mais \#2) (souvent 1) (exemple 1) (a 3) (tête 1) 	... 	\\
	p-1 [\#40]	& (du \#601) (quart 11) (autre \#12) (un \#108) (le \#531) (de \#54) ... 			\\
	p+1 [\#531]	& (numérique \#4) (cherchent \#3) (virtuel \#8) (est \#63) (va \#11) (et \#73) ... 		\\
	p+2 [\#860] 	& (chrétien \#1) (doit \#4) (surtout \#1) (pour \#8) (un \#15) (être \#9) (une \#12)	... 	\\
	\end{tabular}
	}

\vskip-1ex
\item {\scriptsize the [p-1] neighbor distribution of known word "monde"}
	{\tiny
	\begin{tabular}{l|l|l|l|l|l}
			  & [p-1]	& le 	&	un 	&	du 	&	de	\\ \hline
	\textbf{kW=monde} & \#1724	& 0.308	&	0.063 	&	0.349 	&	0.031	\\
	\end{tabular}
	}

\end{itemize}

\end{frame}



%==============================================================================================================
\begin{frame}[t]{Annexe}

\scriptsize 
	\begin{itemize}
	\item[\textbf{3.}] compute the \textbf{\color{purple}KL divergence} between the neighbor distributions \\
		\hskip5ex of all known word (\textit{kW}) and a new word (\textit{nW})

		\vskip-1ex	
		\begin{center}$D_{KL} \left( \ P_{k}(\bullet|\textbf{\color{blendedblue}kW}) \ || \ {P_{k}(\bullet|\textbf{\color{blendedblue}nW})}\ \right) = \sum\limits_{w}^{ } \  P_{k}(w|\textbf{\color{blendedblue}kW}) \ log \left( \frac{P_{k}(w|\textbf{\color{blendedblue}kW})}{P_{k}(w|\textbf{\color{blendedblue}nW})} \right)$\end{center}
		
	\end{itemize}


\vskip-1ex\rule{\textwidth}{1.15pt}

\scriptsize 
\begin{itemize}
\item {\scriptsize compute the divergence between the [p-1] neighbor distributions}
\end{itemize}
	
	\begin{center}
	{\scriptsize
	\vskip-2ex
	\begin{tabular}{l|l|l|l|l}
	[p-1]		  	& le	& un 	& du 	& de	\\ \hline
	\textbf{nW=tournevis}	& 0.333 & 0.333 & 0.167 & 0.167	\\
	\textbf{kW=monde}  	& 0.308 & 0.063 & 0.349 & 0.031	\\
	\end{tabular}
	}
	\end{center}

\only<2>
{
	\vskip2.5ex
	{\scriptsize
	\begin{tabular}{l|l}
	[p-1]		  		& \textbf{$D(le)$}   \\ \hline \hhline{--}
	\textbf{kW=monde,nW=tournevis}	& \textbf{-0.035}    \\
	\end{tabular}
	}

	\begin{center}
	\vskip2.5ex
	\textbf{w = le}
	\vskip1.5ex	
	
	{\scriptsize
	\begin{tabular}{l}
	$D(w)={P\left ( w|kW \right )} log_{2}\left ( \frac{P\left ( w|kW \right )}{P\left ( w|nW \right )} \right )$\\
	$D(w)=0.308 log_{2}\left ( \frac{0.308}{0.333} \right )$\\
	$D(w)=-0.035$\\
	\end{tabular}
	}
	\end{center}
}

\only<3>
{
	\vskip2.5ex
	{\scriptsize
	\begin{tabular}{l|ll}
	[p-1]		  		& $D(le)$   & \textbf{$D(un)$} \\ \hline \hhline{---}
	\textbf{kW=monde,nW=tournevis}	& -0.035    & \textbf{-0.151}  \\
	\end{tabular}
	}
	
	\begin{center}
	\vskip3.05ex	
	\textbf{w = un}
	\vskip1.5ex	
	
	{\scriptsize
	\begin{tabular}{l}
	$D(w)={P\left ( w|kW \right )} log_{2}\left ( \frac{P\left ( w|kW \right )}{P\left ( w|nW \right )} \right )$\\
	$D(w)=0.063 log_{2}\left ( \frac{0.063}{0.333} \right )$\\	
	$D(w)=-0.151$\\
	\end{tabular}
	}
	\end{center}
}
		
\only<4>
{
	\vskip2.5ex
	{\scriptsize
	\begin{tabular}{l|lll}
	[p-1]		  		& $D(le)$  & $D(un)$   & \textbf{$D(du)$}  \\ \hline \hhline{----}
	\textbf{kW=monde,nW=tournevis}	& -0.035   & -0.151    & \textbf{0.371}  \\
	\end{tabular}
	}
	
	\begin{center}
	\vskip2.5ex
	\textbf{w = du}
	\vskip1.5ex	
	
	{\scriptsize
	\begin{tabular}{l}
	$D(w)={P\left ( w|kW \right )} log_{2}\left ( \frac{P\left ( w|kW \right )}{P\left ( w|nW \right )} \right )$\\
	$D(w)=0.349 log_{2}\left ( \frac{0.349}{0.167} \right )$\\	
	$D(w)=0.371$\\
	\end{tabular}
	}
	\end{center}
}

\only<5>
{
	\vskip2.5ex
	{\scriptsize
	\begin{tabular}{l|llll}
	[p-1]		  		& $D(le)$   & $D(un)$   & $D(du)$   & \textbf{$D(de)$}	\\ \hline \hhline{-----}
	\textbf{kW=monde,nW=tournevis}	& -0.035    & -0.151    & 0.371   & \textbf{-0.075}	 \\
	\end{tabular}
	}
	
	\begin{center}
	\vskip2.5ex
	\textbf{w = de}
	\vskip1.5ex	
	
	{\scriptsize
	\begin{tabular}{l}
	$D(w)={P\left ( w|kW \right )} log_{2}\left ( \frac{P\left ( w|kW \right )}{P\left ( w|nW \right )} \right )$\\
	$D(w)=0.031 log_{2}\left ( \frac{0.031}{0.167} \right )$	\\
	$D(w)=-0.075$\\
	\end{tabular}
	}
	\end{center}
}

\only<6->
{
	\vskip2.5ex
	{\scriptsize
	\begin{tabular}{l|lllll}
	[p-1]		  		& $D(le)$  & $D(un)$  & $D(du)$  & $D(de)$	& \cellcolor{blue!25} $D{_{KL}}(p||q)$ 	\\ \hline \hhline{------}
	\textbf{kW=monde,nW=tournevis}	& -0.035   & -0.151   & 0.371	 & -0.075	& \cellcolor{blue!25} \textbf{0.110}	\\ 
	\end{tabular}
	}
}
	
\end{frame}



%==============================================================================================================
\begin{frame}[t]{Annexe}

\scriptsize 
	\begin{itemize}
	\item[\textbf{4.}] select \textbf{\color{purple}the most similar words} to a new word with respect to the KL divergences	
		\vskip-1ex
		\begin{center}
		$D(\textbf{\color{blendedblue}kW},\textbf{\color{blendedblue}nW}) = \sum\limits_{k}^{ } D_{k}(\textbf{\color{blendedblue}kW},\textbf{\color{blendedblue}nW})$ 
		\end{center}
		
	\end{itemize}


\vskip-1ex\rule{\textwidth}{1.15pt}

	\vskip2.5ex
	{\footnotesize Example : Wikipedia corpus, 3-grams}
	
	\vskip2.5ex
	{\scriptsize
	\begin{tabular}{l|lllll}
	\textbf{new word - known word} 	& \textbf{Total}  & \textbf{[p-2]}  & \textbf{[p-1]}	& \textbf{[p+1]} & \textbf{[p+2]}  \\ \hline
	tournevis-système		& 26.8459	& 10.6792	& 0.7044	& 7.9961	& 7.4662	\\ \hline
	tournevis-jeu			& 27.6926	& 10.6276	& 0.4276	& 9.0107	& 7.6267	\\ \hline
	tournevis-modèle		& 28.3001	& 11.5795	& 0.7768	& 8.0591	& 7.8847	\\ \hline
	tournevis-véhicule		& 28.482	& 12.411	& 0.706		& 8.2637	& 7.1013	\\ \hline
	tournevis-traitement		& 29.0743	& 11.7698	& 0.5091	& 8.8681	& 7.9273	\\ \hline
	tournevis-courant		& 29.3598	& 10.9703	& 1.2209	& 9.1505	& 8.0181	\\ \hline
	tournevis-type			& 29.499	& 11.1394	& 1.445		& 8.6027	& 8.3119	\\ \hline
	tournevis-poisson		& 29.5627	& 11.9312	& 0.5941	& 9.6137	& 7.4237	\\ \hline
	tournevis-style			& 29.6316	& 11.3593	& 0.6154	& 9.5178	& 8.1391	\\ \hline
	tournevis-dispositif		& 29.6418	& 12.0949	& 0.8052	& 9.2124	& 7.5293	\\ \hline
	\end{tabular}
	}

\end{frame}



%==============================================================================================================
\begin{frame}[t]{Annexe: Add a new word nW into a language model}

{\scriptsize

\begin{algorithmic}[1]
\STATE {\color{blue}newLM} $\leftarrow$ LM
\STATE {\color{blue}newNgrams} $\leftarrow$ $\varnothing$
\STATE \emph{\color{darkgray}\# process the reference ngrams}
\FOR {\textbf{each} ngram $\in$ LM}
	\FOR {\textbf{each} kW $\in$ similarWords(\textbf{\color{red}nW})}
		\IF {$contains$(ngram, kW)}		
			\STATE ngram\textbf{'} $\leftarrow$ $replace$(ngram, kW, nW)
			\STATE push({\color{blue}newNgrams}, ngram\textbf{'})
		\ENDIF
	\ENDFOR
\ENDFOR

\STATE \emph{\color{darkgray}\# choose the new ngrams to add to the newLM}
\STATE S  $\leftarrow$ $getUniqueSequences$({\color{blue}newNgrams})
\FOR {\textbf{each} seq $\in$ S}
	\IF {$frequency$(seq) = 1}
		\STATE prob  $\leftarrow$ $getProbability$(seq)
	\ELSE
		\STATE P  $\leftarrow$ $getProbabilities$(seq)
		\STATE prob $\leftarrow$ $medianProbability$(P)
	\ENDIF
	\STATE push({\color{blue}newLM},  ``prob seq'')
\ENDFOR
\end{algorithmic}
}


\end{frame}



\end{document}


